{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58acdb2c-e350-49b3-bdd9-482ed050e301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0aca34-a09a-4041-a959-4444f2827f7b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-5.1\", api_key=\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d109ce3f-c681-4428-8bd3-5ef2a143ebb7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "tavily_search = TavilySearchResults(max_results=2, tavily_api_key=\"YOUR_TAVILY_API_KEY\")\n",
    "\n",
    "@tool\n",
    "def get_current_date():\n",
    "    \"\"\"Return the current date and time. Use this tool first for any time-based queries.\"\"\"\n",
    "    return f\"The current date is: {datetime('%d %B %Y')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4fdf62-5682-4459-b6ab-abbf7e7d2812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tools = [tavily_search, get_current_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1678fa6-43ca-452f-906e-cfbc7b7ee7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5eae44-5f2a-4940-a317-543ecf5d5f25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad922351-e6b2-4a56-b944-a3817bc29020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"message\": [llm_with_tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a496f02-cd66-45a9-a101-4c70bbeb2a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb07bf1e-a50c-49b6-a897-dac8c53bc19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0xff630dba8950>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7d0c59b-7f56-4e0e-8da0-7a222a44733a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0xff630dba8950>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62e8206-8869-4de0-9f7a-f59b01781375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0xff630dba8950>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b97ca4-604c-44a2-9ab7-5915fd785105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0xff630dba8950>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_edge(\"tools\", \"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ed4357-435a-474a-bdda-7207d9719dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0xff630dba8950>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0cc75ee-7653-4d46-90c8-8de94a54a9fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248b7862-a785-4b1f-92a5-428ff169b09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cabb55f3-49b8-4bc7-b699-e39f3ba1a21a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 16"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def render_markdown(md_string):\n",
    "    display(Markdown(md_string))\n",
    "\n",
    "def process_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "    return message\n",
    "\n",
    "def process_query(query, config=None):\n",
    "    inputs = {\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "    message = process_stream(graph.stream(inputs, config, stream_mode=\"values\"))\n",
    "    render_markdown(f\"## Answer:\\n{message.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a641d1d5-e319-4805-aeb3-68105c1c29d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001B[1m Human Message \u001B[0m=================================\n\nHello! What can you do ?\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4929623714803098>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m process_query(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello! What can you do ?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m<command-5253633435632077>, line 17\u001B[0m, in \u001B[0;36mprocess_query\u001B[0;34m(query, config)\u001B[0m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_query\u001B[39m(query, config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n",
       "\u001B[1;32m     16\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: query}]}\n",
       "\u001B[0;32m---> 17\u001B[0m     message \u001B[38;5;241m=\u001B[39m process_stream(graph\u001B[38;5;241m.\u001B[39mstream(inputs, config, stream_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[1;32m     18\u001B[0m     render_markdown(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m## Answer:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmessage\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m<command-5253633435632077>, line 7\u001B[0m, in \u001B[0;36mprocess_stream\u001B[0;34m(stream)\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_stream\u001B[39m(stream):\n",
       "\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m stream:\n",
       "\u001B[1;32m      8\u001B[0m         message \u001B[38;5;241m=\u001B[39m s[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
       "\u001B[1;32m      9\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(message, \u001B[38;5;28mtuple\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langgraph/pregel/main.py:2646\u001B[0m, in \u001B[0;36mPregel.stream\u001B[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   2644\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m task \u001B[38;5;129;01min\u001B[39;00m loop\u001B[38;5;241m.\u001B[39mmatch_cached_writes():\n",
       "\u001B[1;32m   2645\u001B[0m     loop\u001B[38;5;241m.\u001B[39moutput_writes(task\u001B[38;5;241m.\u001B[39mid, task\u001B[38;5;241m.\u001B[39mwrites, cached\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[0;32m-> 2646\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m runner\u001B[38;5;241m.\u001B[39mtick(\n",
       "\u001B[1;32m   2647\u001B[0m     [t \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m loop\u001B[38;5;241m.\u001B[39mtasks\u001B[38;5;241m.\u001B[39mvalues() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m t\u001B[38;5;241m.\u001B[39mwrites],\n",
       "\u001B[1;32m   2648\u001B[0m     timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_timeout,\n",
       "\u001B[1;32m   2649\u001B[0m     get_waiter\u001B[38;5;241m=\u001B[39mget_waiter,\n",
       "\u001B[1;32m   2650\u001B[0m     schedule_task\u001B[38;5;241m=\u001B[39mloop\u001B[38;5;241m.\u001B[39maccept_push,\n",
       "\u001B[1;32m   2651\u001B[0m ):\n",
       "\u001B[1;32m   2652\u001B[0m     \u001B[38;5;66;03m# emit output\u001B[39;00m\n",
       "\u001B[1;32m   2653\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _output(\n",
       "\u001B[1;32m   2654\u001B[0m         stream_mode, print_mode, subgraphs, stream\u001B[38;5;241m.\u001B[39mget, queue\u001B[38;5;241m.\u001B[39mEmpty\n",
       "\u001B[1;32m   2655\u001B[0m     )\n",
       "\u001B[1;32m   2656\u001B[0m loop\u001B[38;5;241m.\u001B[39mafter_tick()\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langgraph/pregel/_runner.py:167\u001B[0m, in \u001B[0;36mPregelRunner.tick\u001B[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001B[0m\n",
       "\u001B[1;32m    165\u001B[0m t \u001B[38;5;241m=\u001B[39m tasks[\u001B[38;5;241m0\u001B[39m]\n",
       "\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 167\u001B[0m     run_with_retry(\n",
       "\u001B[1;32m    168\u001B[0m         t,\n",
       "\u001B[1;32m    169\u001B[0m         retry_policy,\n",
       "\u001B[1;32m    170\u001B[0m         configurable\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[1;32m    171\u001B[0m             CONFIG_KEY_CALL: partial(\n",
       "\u001B[1;32m    172\u001B[0m                 _call,\n",
       "\u001B[1;32m    173\u001B[0m                 weakref\u001B[38;5;241m.\u001B[39mref(t),\n",
       "\u001B[1;32m    174\u001B[0m                 retry_policy\u001B[38;5;241m=\u001B[39mretry_policy,\n",
       "\u001B[1;32m    175\u001B[0m                 futures\u001B[38;5;241m=\u001B[39mweakref\u001B[38;5;241m.\u001B[39mref(futures),\n",
       "\u001B[1;32m    176\u001B[0m                 schedule_task\u001B[38;5;241m=\u001B[39mschedule_task,\n",
       "\u001B[1;32m    177\u001B[0m                 submit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubmit,\n",
       "\u001B[1;32m    178\u001B[0m             ),\n",
       "\u001B[1;32m    179\u001B[0m         },\n",
       "\u001B[1;32m    180\u001B[0m     )\n",
       "\u001B[1;32m    181\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommit(t, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001B[0m, in \u001B[0;36mrun_with_retry\u001B[0;34m(task, retry_policy, configurable)\u001B[0m\n",
       "\u001B[1;32m     40\u001B[0m     task\u001B[38;5;241m.\u001B[39mwrites\u001B[38;5;241m.\u001B[39mclear()\n",
       "\u001B[1;32m     41\u001B[0m     \u001B[38;5;66;03m# run the task\u001B[39;00m\n",
       "\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m task\u001B[38;5;241m.\u001B[39mproc\u001B[38;5;241m.\u001B[39minvoke(task\u001B[38;5;241m.\u001B[39minput, config)\n",
       "\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ParentCommand \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
       "\u001B[1;32m     44\u001B[0m     ns: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:656\u001B[0m, in \u001B[0;36mRunnableSeq.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    654\u001B[0m     \u001B[38;5;66;03m# run in context\u001B[39;00m\n",
       "\u001B[1;32m    655\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(config, run) \u001B[38;5;28;01mas\u001B[39;00m context:\n",
       "\u001B[0;32m--> 656\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    657\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    658\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001B[0m, in \u001B[0;36mRunnableCallable.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    398\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(ret)\n",
       "\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 400\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecurse \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, Runnable):\n",
       "\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
       "\n",
       "File \u001B[0;32m<command-5253633435632068>, line 2\u001B[0m, in \u001B[0;36mchatbot\u001B[0;34m(state)\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchatbot\u001B[39m(state: State):\n",
       "\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m: [llm_with_tools\u001B[38;5;241m.\u001B[39minvoke(state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m])]}\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_core/runnables/base.py:5691\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   5684\u001B[0m \u001B[38;5;129m@override\u001B[39m\n",
       "\u001B[1;32m   5685\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n",
       "\u001B[1;32m   5686\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   5689\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m   5690\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n",
       "\u001B[0;32m-> 5691\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n",
       "\u001B[1;32m   5692\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n",
       "\u001B[1;32m   5693\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n",
       "\u001B[1;32m   5694\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n",
       "\u001B[1;32m   5695\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:402\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    388\u001B[0m \u001B[38;5;129m@override\u001B[39m\n",
       "\u001B[1;32m    389\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n",
       "\u001B[1;32m    390\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    395\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n",
       "\u001B[1;32m    396\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AIMessage:\n",
       "\u001B[1;32m    397\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n",
       "\u001B[1;32m    398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n",
       "\u001B[1;32m    399\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAIMessage\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    400\u001B[0m         cast(\n",
       "\u001B[1;32m    401\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChatGeneration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m--> 402\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n",
       "\u001B[1;32m    403\u001B[0m                 [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n",
       "\u001B[1;32m    404\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n",
       "\u001B[1;32m    405\u001B[0m                 callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    406\u001B[0m                 tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    407\u001B[0m                 metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    408\u001B[0m                 run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    409\u001B[0m                 run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n",
       "\u001B[1;32m    410\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n",
       "\u001B[1;32m    411\u001B[0m             )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n",
       "\u001B[1;32m    412\u001B[0m         )\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m    413\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1121\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1112\u001B[0m \u001B[38;5;129m@override\u001B[39m\n",
       "\u001B[1;32m   1113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n",
       "\u001B[1;32m   1114\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1118\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n",
       "\u001B[1;32m   1119\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n",
       "\u001B[1;32m   1120\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n",
       "\u001B[0;32m-> 1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:931\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    928\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n",
       "\u001B[1;32m    929\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    930\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n",
       "\u001B[0;32m--> 931\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n",
       "\u001B[1;32m    932\u001B[0m                 m,\n",
       "\u001B[1;32m    933\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n",
       "\u001B[1;32m    934\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    935\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n",
       "\u001B[1;32m    936\u001B[0m             )\n",
       "\u001B[1;32m    937\u001B[0m         )\n",
       "\u001B[1;32m    938\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    939\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1233\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1231\u001B[0m     result \u001B[38;5;241m=\u001B[39m generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n",
       "\u001B[1;32m   1232\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[0;32m-> 1233\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n",
       "\u001B[1;32m   1234\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n",
       "\u001B[1;32m   1235\u001B[0m     )\n",
       "\u001B[1;32m   1236\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1237\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1386\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1384\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp_response\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m   1385\u001B[0m         e\u001B[38;5;241m.\u001B[39mresponse \u001B[38;5;241m=\u001B[39m raw_response\u001B[38;5;241m.\u001B[39mhttp_response  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
       "\u001B[0;32m-> 1386\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m   1387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n",
       "\u001B[1;32m   1388\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minclude_response_headers\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1390\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1391\u001B[0m ):\n",
       "\u001B[1;32m   1392\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response\u001B[38;5;241m.\u001B[39mheaders)}\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1381\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1374\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _construct_lc_result_from_responses_api(\n",
       "\u001B[1;32m   1375\u001B[0m             response,\n",
       "\u001B[1;32m   1376\u001B[0m             schema\u001B[38;5;241m=\u001B[39moriginal_schema_obj,\n",
       "\u001B[1;32m   1377\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mgeneration_info,\n",
       "\u001B[1;32m   1378\u001B[0m             output_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_version,\n",
       "\u001B[1;32m   1379\u001B[0m         )\n",
       "\u001B[1;32m   1380\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1381\u001B[0m         raw_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mwith_raw_response\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpayload)\n",
       "\u001B[1;32m   1382\u001B[0m         response \u001B[38;5;241m=\u001B[39m raw_response\u001B[38;5;241m.\u001B[39mparse()\n",
       "\u001B[1;32m   1383\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001B[0m, in \u001B[0;36mto_raw_response_wrapper.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    360\u001B[0m extra_headers[RAW_RESPONSE_HEADER] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    362\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mextra_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m extra_headers\n",
       "\u001B[0;32m--> 364\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cast(LegacyAPIResponse[R], func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    284\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n",
       "\u001B[0;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n",
       "\u001B[1;32m   1145\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n",
       "\u001B[1;32m   1147\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1189\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m not_given,\n",
       "\u001B[1;32m   1190\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n",
       "\u001B[1;32m   1191\u001B[0m     validate_response_format(response_format)\n",
       "\u001B[0;32m-> 1192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n",
       "\u001B[1;32m   1193\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/chat/completions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1194\u001B[0m         body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n",
       "\u001B[1;32m   1195\u001B[0m             {\n",
       "\u001B[1;32m   1196\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n",
       "\u001B[1;32m   1197\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n",
       "\u001B[1;32m   1198\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maudio\u001B[39m\u001B[38;5;124m\"\u001B[39m: audio,\n",
       "\u001B[1;32m   1199\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrequency_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: frequency_penalty,\n",
       "\u001B[1;32m   1200\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m: function_call,\n",
       "\u001B[1;32m   1201\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctions\u001B[39m\u001B[38;5;124m\"\u001B[39m: functions,\n",
       "\u001B[1;32m   1202\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogit_bias\u001B[39m\u001B[38;5;124m\"\u001B[39m: logit_bias,\n",
       "\u001B[1;32m   1203\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: logprobs,\n",
       "\u001B[1;32m   1204\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_completion_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_completion_tokens,\n",
       "\u001B[1;32m   1205\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n",
       "\u001B[1;32m   1206\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n",
       "\u001B[1;32m   1207\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodalities\u001B[39m\u001B[38;5;124m\"\u001B[39m: modalities,\n",
       "\u001B[1;32m   1208\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m: n,\n",
       "\u001B[1;32m   1209\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparallel_tool_calls\u001B[39m\u001B[38;5;124m\"\u001B[39m: parallel_tool_calls,\n",
       "\u001B[1;32m   1210\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m: prediction,\n",
       "\u001B[1;32m   1211\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpresence_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: presence_penalty,\n",
       "\u001B[1;32m   1212\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt_cache_key\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt_cache_key,\n",
       "\u001B[1;32m   1213\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt_cache_retention\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt_cache_retention,\n",
       "\u001B[1;32m   1214\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreasoning_effort\u001B[39m\u001B[38;5;124m\"\u001B[39m: reasoning_effort,\n",
       "\u001B[1;32m   1215\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n",
       "\u001B[1;32m   1216\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msafety_identifier\u001B[39m\u001B[38;5;124m\"\u001B[39m: safety_identifier,\n",
       "\u001B[1;32m   1217\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m: seed,\n",
       "\u001B[1;32m   1218\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mservice_tier\u001B[39m\u001B[38;5;124m\"\u001B[39m: service_tier,\n",
       "\u001B[1;32m   1219\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop,\n",
       "\u001B[1;32m   1220\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstore\u001B[39m\u001B[38;5;124m\"\u001B[39m: store,\n",
       "\u001B[1;32m   1221\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n",
       "\u001B[1;32m   1222\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream_options\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream_options,\n",
       "\u001B[1;32m   1223\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n",
       "\u001B[1;32m   1224\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n",
       "\u001B[1;32m   1225\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n",
       "\u001B[1;32m   1226\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_logprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_logprobs,\n",
       "\u001B[1;32m   1227\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n",
       "\u001B[1;32m   1228\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m: user,\n",
       "\u001B[1;32m   1229\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mverbosity\u001B[39m\u001B[38;5;124m\"\u001B[39m: verbosity,\n",
       "\u001B[1;32m   1230\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweb_search_options\u001B[39m\u001B[38;5;124m\"\u001B[39m: web_search_options,\n",
       "\u001B[1;32m   1231\u001B[0m             },\n",
       "\u001B[1;32m   1232\u001B[0m             completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParamsStreaming\n",
       "\u001B[1;32m   1233\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m stream\n",
       "\u001B[1;32m   1234\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParamsNonStreaming,\n",
       "\u001B[1;32m   1235\u001B[0m         ),\n",
       "\u001B[1;32m   1236\u001B[0m         options\u001B[38;5;241m=\u001B[39mmake_request_options(\n",
       "\u001B[1;32m   1237\u001B[0m             extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n",
       "\u001B[1;32m   1238\u001B[0m         ),\n",
       "\u001B[1;32m   1239\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mChatCompletion,\n",
       "\u001B[1;32m   1240\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m   1241\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mStream[ChatCompletionChunk],\n",
       "\u001B[1;32m   1242\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/openai/_base_client.py:1297\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001B[0m\n",
       "\u001B[1;32m   1288\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n",
       "\u001B[1;32m   1289\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease pass raw bytes via the `content` parameter instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1291\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n",
       "\u001B[1;32m   1292\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n",
       "\u001B[1;32m   1293\u001B[0m     )\n",
       "\u001B[1;32m   1294\u001B[0m opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n",
       "\u001B[1;32m   1295\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, content\u001B[38;5;241m=\u001B[39mcontent, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n",
       "\u001B[1;32m   1296\u001B[0m )\n",
       "\u001B[0;32m-> 1297\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/openai/_base_client.py:1070\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n",
       "\u001B[1;32m   1067\u001B[0m             err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n",
       "\u001B[1;32m   1069\u001B[0m         log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m-> 1070\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1072\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
       "\u001B[1;32m   1074\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcould not resolve response (should never happen)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "\u001B[0;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
       "\u001B[0mDuring task with name 'chatbot' and id '1b1e92da-c7fc-7741-48a1-3ca137f5da60'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RateLimitError",
        "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>RateLimitError</span>: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "USER_SESSION_SCOPE_LIBRARY_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KAN01",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-4929623714803098>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m process_query(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello! What can you do ?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m<command-5253633435632077>, line 17\u001B[0m, in \u001B[0;36mprocess_query\u001B[0;34m(query, config)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_query\u001B[39m(query, config\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     16\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: query}]}\n\u001B[0;32m---> 17\u001B[0m     message \u001B[38;5;241m=\u001B[39m process_stream(graph\u001B[38;5;241m.\u001B[39mstream(inputs, config, stream_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     18\u001B[0m     render_markdown(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m## Answer:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmessage\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m<command-5253633435632077>, line 7\u001B[0m, in \u001B[0;36mprocess_stream\u001B[0;34m(stream)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_stream\u001B[39m(stream):\n\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m stream:\n\u001B[1;32m      8\u001B[0m         message \u001B[38;5;241m=\u001B[39m s[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(message, \u001B[38;5;28mtuple\u001B[39m):\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langgraph/pregel/main.py:2646\u001B[0m, in \u001B[0;36mPregel.stream\u001B[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001B[0m\n\u001B[1;32m   2644\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m task \u001B[38;5;129;01min\u001B[39;00m loop\u001B[38;5;241m.\u001B[39mmatch_cached_writes():\n\u001B[1;32m   2645\u001B[0m     loop\u001B[38;5;241m.\u001B[39moutput_writes(task\u001B[38;5;241m.\u001B[39mid, task\u001B[38;5;241m.\u001B[39mwrites, cached\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m-> 2646\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m runner\u001B[38;5;241m.\u001B[39mtick(\n\u001B[1;32m   2647\u001B[0m     [t \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m loop\u001B[38;5;241m.\u001B[39mtasks\u001B[38;5;241m.\u001B[39mvalues() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m t\u001B[38;5;241m.\u001B[39mwrites],\n\u001B[1;32m   2648\u001B[0m     timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_timeout,\n\u001B[1;32m   2649\u001B[0m     get_waiter\u001B[38;5;241m=\u001B[39mget_waiter,\n\u001B[1;32m   2650\u001B[0m     schedule_task\u001B[38;5;241m=\u001B[39mloop\u001B[38;5;241m.\u001B[39maccept_push,\n\u001B[1;32m   2651\u001B[0m ):\n\u001B[1;32m   2652\u001B[0m     \u001B[38;5;66;03m# emit output\u001B[39;00m\n\u001B[1;32m   2653\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _output(\n\u001B[1;32m   2654\u001B[0m         stream_mode, print_mode, subgraphs, stream\u001B[38;5;241m.\u001B[39mget, queue\u001B[38;5;241m.\u001B[39mEmpty\n\u001B[1;32m   2655\u001B[0m     )\n\u001B[1;32m   2656\u001B[0m loop\u001B[38;5;241m.\u001B[39mafter_tick()\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langgraph/pregel/_runner.py:167\u001B[0m, in \u001B[0;36mPregelRunner.tick\u001B[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001B[0m\n\u001B[1;32m    165\u001B[0m t \u001B[38;5;241m=\u001B[39m tasks[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 167\u001B[0m     run_with_retry(\n\u001B[1;32m    168\u001B[0m         t,\n\u001B[1;32m    169\u001B[0m         retry_policy,\n\u001B[1;32m    170\u001B[0m         configurable\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m    171\u001B[0m             CONFIG_KEY_CALL: partial(\n\u001B[1;32m    172\u001B[0m                 _call,\n\u001B[1;32m    173\u001B[0m                 weakref\u001B[38;5;241m.\u001B[39mref(t),\n\u001B[1;32m    174\u001B[0m                 retry_policy\u001B[38;5;241m=\u001B[39mretry_policy,\n\u001B[1;32m    175\u001B[0m                 futures\u001B[38;5;241m=\u001B[39mweakref\u001B[38;5;241m.\u001B[39mref(futures),\n\u001B[1;32m    176\u001B[0m                 schedule_task\u001B[38;5;241m=\u001B[39mschedule_task,\n\u001B[1;32m    177\u001B[0m                 submit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubmit,\n\u001B[1;32m    178\u001B[0m             ),\n\u001B[1;32m    179\u001B[0m         },\n\u001B[1;32m    180\u001B[0m     )\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommit(t, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001B[0m, in \u001B[0;36mrun_with_retry\u001B[0;34m(task, retry_policy, configurable)\u001B[0m\n\u001B[1;32m     40\u001B[0m     task\u001B[38;5;241m.\u001B[39mwrites\u001B[38;5;241m.\u001B[39mclear()\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;66;03m# run the task\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m task\u001B[38;5;241m.\u001B[39mproc\u001B[38;5;241m.\u001B[39minvoke(task\u001B[38;5;241m.\u001B[39minput, config)\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ParentCommand \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     44\u001B[0m     ns: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:656\u001B[0m, in \u001B[0;36mRunnableSeq.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    654\u001B[0m     \u001B[38;5;66;03m# run in context\u001B[39;00m\n\u001B[1;32m    655\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(config, run) \u001B[38;5;28;01mas\u001B[39;00m context:\n\u001B[0;32m--> 656\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    657\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    658\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001B[0m, in \u001B[0;36mRunnableCallable.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    398\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(ret)\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 400\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecurse \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, Runnable):\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
        "File \u001B[0;32m<command-5253633435632068>, line 2\u001B[0m, in \u001B[0;36mchatbot\u001B[0;34m(state)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchatbot\u001B[39m(state: State):\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m: [llm_with_tools\u001B[38;5;241m.\u001B[39minvoke(state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m])]}\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_core/runnables/base.py:5691\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   5684\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m   5685\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m   5686\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5689\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5690\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[0;32m-> 5691\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[1;32m   5692\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   5693\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[1;32m   5694\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[1;32m   5695\u001B[0m     )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:402\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    389\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    395\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    396\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m AIMessage:\n\u001B[1;32m    397\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    399\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAIMessage\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    400\u001B[0m         cast(\n\u001B[1;32m    401\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChatGeneration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m--> 402\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[1;32m    403\u001B[0m                 [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[1;32m    404\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    405\u001B[0m                 callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    406\u001B[0m                 tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    407\u001B[0m                 metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    408\u001B[0m                 run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    409\u001B[0m                 run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m    410\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    411\u001B[0m             )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    412\u001B[0m         )\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m    413\u001B[0m     )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1121\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m   1112\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m   1113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m   1114\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1118\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m   1119\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m   1120\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m-> 1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:931\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    928\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[1;32m    929\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    930\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 931\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[1;32m    932\u001B[0m                 m,\n\u001B[1;32m    933\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    934\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    935\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    936\u001B[0m             )\n\u001B[1;32m    937\u001B[0m         )\n\u001B[1;32m    938\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    939\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1233\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1231\u001B[0m     result \u001B[38;5;241m=\u001B[39m generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[1;32m   1232\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1233\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[1;32m   1234\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m   1235\u001B[0m     )\n\u001B[1;32m   1236\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1237\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1386\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1384\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp_response\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1385\u001B[0m         e\u001B[38;5;241m.\u001B[39mresponse \u001B[38;5;241m=\u001B[39m raw_response\u001B[38;5;241m.\u001B[39mhttp_response  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m-> 1386\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m   1387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1388\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minclude_response_headers\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1390\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1391\u001B[0m ):\n\u001B[1;32m   1392\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response\u001B[38;5;241m.\u001B[39mheaders)}\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1381\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1374\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _construct_lc_result_from_responses_api(\n\u001B[1;32m   1375\u001B[0m             response,\n\u001B[1;32m   1376\u001B[0m             schema\u001B[38;5;241m=\u001B[39moriginal_schema_obj,\n\u001B[1;32m   1377\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mgeneration_info,\n\u001B[1;32m   1378\u001B[0m             output_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_version,\n\u001B[1;32m   1379\u001B[0m         )\n\u001B[1;32m   1380\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1381\u001B[0m         raw_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mwith_raw_response\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpayload)\n\u001B[1;32m   1382\u001B[0m         response \u001B[38;5;241m=\u001B[39m raw_response\u001B[38;5;241m.\u001B[39mparse()\n\u001B[1;32m   1383\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001B[0m, in \u001B[0;36mto_raw_response_wrapper.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m extra_headers[RAW_RESPONSE_HEADER] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    362\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mextra_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m extra_headers\n\u001B[0;32m--> 364\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cast(LegacyAPIResponse[R], func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    284\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m   1147\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1189\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m not_given,\n\u001B[1;32m   1190\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m   1191\u001B[0m     validate_response_format(response_format)\n\u001B[0;32m-> 1192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/chat/completions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1194\u001B[0m         body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[1;32m   1195\u001B[0m             {\n\u001B[1;32m   1196\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[1;32m   1197\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[1;32m   1198\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maudio\u001B[39m\u001B[38;5;124m\"\u001B[39m: audio,\n\u001B[1;32m   1199\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrequency_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: frequency_penalty,\n\u001B[1;32m   1200\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m: function_call,\n\u001B[1;32m   1201\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctions\u001B[39m\u001B[38;5;124m\"\u001B[39m: functions,\n\u001B[1;32m   1202\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogit_bias\u001B[39m\u001B[38;5;124m\"\u001B[39m: logit_bias,\n\u001B[1;32m   1203\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: logprobs,\n\u001B[1;32m   1204\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_completion_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_completion_tokens,\n\u001B[1;32m   1205\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[1;32m   1206\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m   1207\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodalities\u001B[39m\u001B[38;5;124m\"\u001B[39m: modalities,\n\u001B[1;32m   1208\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m: n,\n\u001B[1;32m   1209\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparallel_tool_calls\u001B[39m\u001B[38;5;124m\"\u001B[39m: parallel_tool_calls,\n\u001B[1;32m   1210\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m\"\u001B[39m: prediction,\n\u001B[1;32m   1211\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpresence_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: presence_penalty,\n\u001B[1;32m   1212\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt_cache_key\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt_cache_key,\n\u001B[1;32m   1213\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt_cache_retention\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt_cache_retention,\n\u001B[1;32m   1214\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreasoning_effort\u001B[39m\u001B[38;5;124m\"\u001B[39m: reasoning_effort,\n\u001B[1;32m   1215\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n\u001B[1;32m   1216\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msafety_identifier\u001B[39m\u001B[38;5;124m\"\u001B[39m: safety_identifier,\n\u001B[1;32m   1217\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m: seed,\n\u001B[1;32m   1218\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mservice_tier\u001B[39m\u001B[38;5;124m\"\u001B[39m: service_tier,\n\u001B[1;32m   1219\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop,\n\u001B[1;32m   1220\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstore\u001B[39m\u001B[38;5;124m\"\u001B[39m: store,\n\u001B[1;32m   1221\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[1;32m   1222\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream_options\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream_options,\n\u001B[1;32m   1223\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[1;32m   1224\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[1;32m   1225\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[1;32m   1226\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_logprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_logprobs,\n\u001B[1;32m   1227\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[1;32m   1228\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m: user,\n\u001B[1;32m   1229\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mverbosity\u001B[39m\u001B[38;5;124m\"\u001B[39m: verbosity,\n\u001B[1;32m   1230\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweb_search_options\u001B[39m\u001B[38;5;124m\"\u001B[39m: web_search_options,\n\u001B[1;32m   1231\u001B[0m             },\n\u001B[1;32m   1232\u001B[0m             completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParamsStreaming\n\u001B[1;32m   1233\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m stream\n\u001B[1;32m   1234\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParamsNonStreaming,\n\u001B[1;32m   1235\u001B[0m         ),\n\u001B[1;32m   1236\u001B[0m         options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[1;32m   1237\u001B[0m             extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m   1238\u001B[0m         ),\n\u001B[1;32m   1239\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mChatCompletion,\n\u001B[1;32m   1240\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1241\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mStream[ChatCompletionChunk],\n\u001B[1;32m   1242\u001B[0m     )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/openai/_base_client.py:1297\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1288\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1289\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease pass raw bytes via the `content` parameter instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1291\u001B[0m         \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[1;32m   1292\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m   1293\u001B[0m     )\n\u001B[1;32m   1294\u001B[0m opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1295\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, content\u001B[38;5;241m=\u001B[39mcontent, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1296\u001B[0m )\n\u001B[0;32m-> 1297\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-0ec7b229-f273-4cd8-a881-028c230a6380/lib/python3.12/site-packages/openai/_base_client.py:1070\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1067\u001B[0m             err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1069\u001B[0m         log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1070\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1072\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m   1074\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcould not resolve response (should never happen)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
        "\u001B[0;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
        "\u001B[0mDuring task with name 'chatbot' and id '1b1e92da-c7fc-7741-48a1-3ca137f5da60'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_query(\"Hello! What can you do ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32d72f0-40b2-490a-85aa-4169a7aa13ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ReAct agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}